{"nbformat_minor": 0, "cells": [{"source": "Analysis Script: Dimension Reduction\nDeveloped by Konstantin Tskhay\nDate: Tue Sep 22 22:18:15 2015\n\nR version 3.2.2 (2015-08-14) -- \"Fire Safety\"\nx86_64-apple-darwin13.4.0 (64-bit)\n\n------------------------------------------\n\n# Dimension Reduction\n## Principal Component Analysis & Exploratory Factor Analysis\n\n### Steps:\n\n1. Import the data\n2. Load necessary packages\n3. Extract necessary data for PCA\n4. Set up all of the preconditions\n5. Run PCA/Visualizations\n6. Interpret the output\n7. Set up for EFA\n8. Run EFA/Visualizations\n9. Interpret the output\n10. Parallel Analysis (Horn, 1965)\n11. Mini-validation\n\n-----------------------------\n\n### *Note*. I would like to encourage you type your code--this practice ensures better retention. :)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#Click in this cell and press Shift + Enter\ninstall.packages(\"psych\")\ninstall.packages(\"ggplot2\")", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "\n<a href=\"https://twitter.com/intent/tweet?original_referer=http%3A%2F%2Fbigdatauniversity.com%2Fevents%2Fadvanced-machine-learning-dimensionality-reduction-with-pca-and-factor-analysis-hands-on%2F&ref_src=twsrc%5Etfw&text=Learning%20%23MachineLearning%20-%20PCA%20and%20Factor%20Analysis%20@BigDataU%20%23bdumeetup%20%23rstats%20%23datascience%20LIVESTREAM%20HERE-%3E&tw_p=tweetbutton&url=http%3A%2F%2Fbit.ly%2F1OozM3N\" target=\"_blank\">\n              <img src=\"https://ibm.box.com/shared/static/q4bnb69qy989gouvbij4cmzmf5y5lk0e.png\", width = 600>\n            </a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 1: Import the data\n\nThe data are in .csv format: 'new_prof_data.csv'\n\nHere is a brief description of the variables:\n\n\n1. **ID** = Observation ID\n2. **Prof.Name** = The name of the professor. Here, Name1 to Name213 are used\n3. **Present** = \u201cPresents the material in an organized, well-planned manner.\u201d\n4. **Explain** = \u201cExplains concepts clearly and with appropriate use of examples.\u201d\n5. **Communi** = \u201cCommunicates enthusiasm and interest in the course material.\u201d\n6. **Teach** = \u201cAll things considered, performs effectively as a university teacher.\u201d\n7. **Workload** = \u201cCompared to other courses at the same level, the workload is\u2026\u201d\n8. **Difficulty** = \u201cCompared to other courses at the same level, the level of difficulty of the material is\u2026\u201d\n9. **learn.Exp** = \u201cThe value of the overall learning experience is\u2026\u201d\n10. **Retake** = \u201cConsidering your experience with this course, and disregarding your need for it to meet program or degree requirements, would you still have taken this course?\u201d\n11. **Inf.** = The aggregate influence score (Interpersonal Charisma Scale)\n12. **Kind** = The aggregate kindness score (Interpersonal Charisma Scale)\n\n\n**_Notes._**\n\n**Q3-Q6 scale**: 1 = extremely poor; 2 = very poor; 3 = poor; 4 = adequate; 5 = good; 6 = very good; 7 = outstanding\n\n**Q7-Q9 scale**: 1 = very low; 2 = low; 3 = below average; 4 = average; 5 = above average; 6 = high; 7 = very high\n\n**Q10 scale**: proportion of people out of 100 who would still take the course considering the experience\n\n**Q11-Q12 scale**: \u201cI am someone who is\u2026\u201d; 1 = strongly disagree; 2 = moderately disagree; 3 = neither agree nor disagree; 4 = moderately agree; 5 = strongly agree\n\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "<br>", "cell_type": "markdown", "metadata": {}}, {"source": "### Get the data", "cell_type": "markdown", "metadata": {}}, {"source": "Copy the link below:", "cell_type": "markdown", "metadata": {}}, {"source": ">` https://share.datascientistworkbench.com/#/api/v1/workbench/10.114.214.21/shares/ZHB9A8OGW3Q0tsz/new_prof_data.csv`", "cell_type": "markdown", "metadata": {}}, {"source": "Paste into search bar:   \n<img src = \"https://ibm.box.com/shared/static/r29ew91i2wl5kovje923z45qdlztubf4.png\", width = 600, align = 'left'>", "cell_type": "markdown", "metadata": {}}, {"source": "<br>", "cell_type": "markdown", "metadata": {}}, {"source": "### Import data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## Read the data into an object named data\ndata <- read.csv('/resources/new_prof_data.csv')\n\n## Examine data:\nnames(data)\nstr(data)\nsummary(data)\nhead(data)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Step 2: Load necessary packages\n\nThis will allow you to use functions needed for factor analysis and principal component analysis\n\nif you don't have these packages on your machine use the following line:\n\n    install.packages('package_name')\n    \nWhere, *package_name* is the name of the package you are interested in", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "library(psych) ## for PCA and EFA \nlibrary(ggplot2) ## for some Visuals", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Step 3: Extract necessary data for PCA\n\nThis step is simple: select relevant columns.\nWe want columns 3 through 8.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "names(data) ## look up index\ncomp.data <- data[,3:8] ## extract data\nnames(comp.data) ## check", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Step 4: Set up all of the preconditions\n\n**Question**: Can we reduce the number of variables?\n**Answer**: Yes. Let's do it.\n\n  \n\nTo do so, we need to see interrelationships between the variables.\nFrom before, we know that there may be 2 types of variables emerging.\n\n1. The variables that specify how good the professors are at communication\n2. The variables that track the course difficulty\n\nLet us examine whether this may be the case", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "round(cor(comp.data), digits = 3) ## produces correlation matrix", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "You can see immediately that all communication variables are highly correlated. The difficulty variable correlates quite highly with the workload variable. However, there appears to be little overlap between communication and workload/difficulty variables.\n\n**This suggests that there are probably 2 components/factors in our data.**\n\nHowever, this example is simple because here we see a separation of the variables in the correlation matrix and know that the variables will probably split *a priori*. The data we work with in the real world are more complex, however.\n\n    e.g., 20, 100, 1000, 10000 variables\n    \nNow, imagine trying to identify how these variables form components or factors using correlation matrix or some type of hypothesis driven reasoning! It is practically impossible!\n\n----------------------------\n### Decision Rules:\n1. Probably 2 components (Communication, Workload)\n2. The components are probably orthogonal\n3. Check it empirically", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 5 & 6: Run PCA/Visualizations & Interpret the output", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "pcaSolution <- prcomp(comp.data, center = TRUE, scale. = TRUE) \n\n## Produced the object with standard deviatons of all variables\n## and Rotation -- or the loadings on the principal components\n\nprint(pcaSolution) ## prints PCA solution\n\npcaSolution$sdev^2", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Let's create the Scree plot: Variance explained versus components", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "plot(pcaSolution, type = \"l\", pch = 16, lwd = 2) ## generates a scree plot", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "This figure will help us to decide how many components we should extract.\nThe first two PC explain most of the variability in the data--so, probably 2 components to extract.\n\nLet's see how important each component is in a different way: Take a look of proportion of variance explained (second line in the output table below)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "summary(pcaSolution)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "0.5669 + 0.2829 ## proportion of matrix variance explained by the \n                ## first 2 components", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### What items fall on each component?\n\nWell, we can take a look at that examining, pcaSolution's matrix:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "pcaSolution$rotation[,1:2] ## only looks at the first 2 components", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**Everything is pretty much as expected!**\n\n**Let's create a visual of the components now:**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "theta <- seq(0,2*pi,length.out = 100)\ncircle <- data.frame(x = cos(theta), y = sin(theta))\np <- ggplot(circle, aes(x,y)) + geom_path()\n\nloadings <- data.frame(pcaSolution$rotation, .names = row.names(pcaSolution$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n        coord_fixed(ratio=1) +\n        labs(x = \"PC1\", y = \"PC2\") +\n        theme_bw()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "This last step is not necessary, however it is a decent visualization. Basically shows that the 2 factors are orthogonal and that the communication variables tend to map onto the first component and that the difficulty variables tend to map on the second component. Naturally, when you have more variables, the circle will be more complex and less conducive to presentation or interpretation. ", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 7: Set up for EFA\n\nOnce again, you want to first extract the relevant data. Typically people will do either PCA or EFA, depending on their research question, which informs the selection of the technique. Factor analysis is used when you think that something bigger than the variables at hand causes those variables and interrelationships between those variables. In other words, you assume that your indicators, measured variables, are not measured perfectly. EFA is very common in survey research, including customer satistaction surveys and personality measurement. \n\n    e.g., suppose your respondents answer some questions about how \"outgoing\" they are and how \"sociable\" they are. We assume further that an underlying personality of Extraversion causes people to choose greater number if they are extraverted and lower numbers if they are introveted! That is, people who are extraverted are expected to say that they are more \"outgoing\" and \"sociable\" than people who are introverted. We say here, then that extraversion construct (factor) causes systematic variance in reponses to items, but also that the measures are not perfect indicators of extraversion. So what is important here, is the shared variance between the items. \n    \n### Factor analysis allows us to identify this latent construct\n\nOkay, now we will:\n\n**1. Extract the relevant data**\n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## note that I am using exactly the same code as in PCA, \n## except that I am naming the object differently\n\nnames(data) ## look up index\nfactor.data <- data[,3:8] ## extract data\nnames(factor.data) ## check", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**2. Decide on the number of factors to extract**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "eigen.values <- eigen(cor(factor.data)) ## extract eigenvalues from cor()\neigen.values ## note that eigenvalues are simply squared st.devs from before! ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Create a new scree plot", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "plot(eigen.values$values, type=\"l\", ylab = 'Unrotated Eigenvalues',\n    xlab = 'Components', lwd = 3)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Once again, we see that 2 factors explain most of the variance. \n\nFurther, you also need to select the factors based on the eigen values themselves they should be greater than 1. Again, the solution is 2. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "eigen.values$values/(sum(eigen.values$values)) ## compute proportion of \n                                               ##total variance\n\n## How much variance due to the first 2 factors?\nsum((eigen.values$values/(sum(eigen.values$values)))[c(1, 2)])", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Okay, we meet the three major standards (see parallel analysis later for an alternative result). ", "cell_type": "markdown", "metadata": {}}, {"source": "**3. What kind of rotation to use?**\n\nHere, we do not truly need to be exactly data driven. All previous work has demonstrated that orthogonal rotation would be preferable. \n\nUse: **Varimax** to increase the distance between factors. \n\n** 4. What would you want as a factoring method?**\n\nHere, we will use principal axis factoring [most commonly used method in psychology research]. \n\nThis says to place factor communalities in the diagonal, which would estimate the residual variation, stating that the factors do not explain all of the variance. In PCA, the 1s are retained across the diagonal. \n\n### Let's Recap the Pre-processing steps & Decisions:\n\n1. Data extracted\n2. Number of factors k = 2\n3. Rotation: Orthogonal rotation\n4. Factoring method: Principal Axis Factoring (PA)\n\nLet's do it!", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 8 & 9: Run EFA/Visualizations & Interpret the output\n\n**Get the correlation matrix**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "corMat <- round(cor(factor.data), digits = 3)\ncorMat", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**Run exploratory factor analysis**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "efaSolution <- fa(r = corMat, nfactors = 2, rotate = \"varimax\", fm = 'pa')\nefaSolution", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "pchisq(.39, 4, ncp = 0, lower.tail = FALSE, log.p = FALSE)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "The model fit appears to be sufficient. Both factor eigen values are above 1. The loadings on each factor are quite high (>.70 threshold) and the proportion of variance explained == 77%. This is a pretty decent solution. \n\nLet's do a quick graph for it! ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "fa.diagram(efaSolution)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "This is not the best graph and it should not be presented in publication, but it is a good representation of what we did.\n\nFactor 1 causes variables Explain, Teach, Present, and Communi\n\nFactor 2 causes variables Workload, Difficulty\n\nName your factors. ", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 10: Parallel Analysis\n\nThis analysis is said to be the most robust analysis for identifying the number of factors. However, it is rarely used. It is rather simple:\n\n1. You will create multiple random datasets with same *n* observations and *k* factors\n2. Compute a correlation matrix for each dataset and extract eigenvalues\n3. If random eigenvalues are larger than those in the PCA or EFA, you know that the values are likely to be simply noise. \n4. So, you want your eigen values to be larger than those simulated randomly.\n\nTypically 100 simulations is recommended, but more is better. You don't want to overload your machine with computations, however. Here is how to do it.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "fa.parallel(factor.data, n.iter = 100)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "A new scree is produced. The red lines represent simulated and resampled data. Blue lines are the actual data. \n\nAs you can see, the line with x-points shows that 2 components should be extracted. -- this line is compared to the top red line and only 2 points exceed the red line--hence, 2 PCs to extract\n\nWhen it comes to FA, there appears a need for a third component--yet, I am skeptical. The third triangle on the blue line pretty much overlaps with the red line. The distances are much larger in the other cases. \n\n** Okay, let's run some FA with 3 factors **", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "efaSolution2 <- fa(r = corMat, nfactors = 3, rotate = \"varimax\", fm = 'pa')\nefaSolution2", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**Output:**\n\nExaminig this output we notice high cross loadings of item Present between PA1 and PA3. Next, the PA3's SS loadings are .81, which does not exceed our threshold = 1. \n\nFinally, there is little information about the model fit, because it is a saturated model. \n\nIn sum, I typically would not advise for retaining the last factor in the current model. ", "cell_type": "markdown", "metadata": {}}, {"source": "## Step 11: Mini-Validation\n\nHere, you want to basically generate factor or component scores with your variables and start building models. I will simply aggregate the variables defined by each component/factor. Then perform linear regression (lm) to see how well my components might predict other variables. \n\nCreate means for Component/factor 1: course communication -- CC\nCreate means for Component/factor 2: course difficulty -- CD\n\nadd them to dataset", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data$CC <- rowMeans(data[, 3:6])\ndata$CD <- rowMeans(data[, 7:8])", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "1. Fit the model examining students' learning experience from communication and difficulty\n2. Fit the model examining students' retake rate (knowing the course and the professor, would they want to take it again)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "fit1 <- lm(learn.Exp ~ CC + CD, data = data); summary(fit1)\nfit2 <- lm(Retake ~ CC + CD, data = data); summary(fit2)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**Output:**\n\nBoth are predictors and explain quite a bit of variance;\n\nHowever, though all students appear to like professors who are better at communication...\n\nThey tend to think taht difficulty produced better learning experience, yet would not take the course again. \n\nNaturally, in more complicated datasets, you can run more complex models. ", "cell_type": "markdown", "metadata": {}}, {"source": "<h1, align=\"center\"> FIN </h1>\n\n\n\n<a href=\"https://twitter.com/intent/tweet?original_referer=http%3A%2F%2Fbigdatauniversity.com%2Fevents%2Fadvanced-machine-learning-dimensionality-reduction-with-pca-and-factor-analysis-hands-on%2F&ref_src=twsrc%5Etfw&text=Just%20learned%20how%20to%20run%20PCA%20and%20FA%20in%20R%23MachineLearning%20@BigDataU%20%23bdumeetup%20%23rstats%20%23datascience%20RECORDING%20HERE-%3E&tw_p=tweetbutton&url=http%3A%2F%2Fbit.ly%2F1OozM3N\" target=\"_blank\">\n              <img src=\"https://ibm.box.com/shared/static/40td5lc1ywwkgwnew845g93vpsjc9poi\", width = 600>\n            </a>\n\n\n## RESOURCES:\n\n\n### Useful Links:\n\n- **Data Science** http://bigdatauniversity.com\n- **Clustering** http://bigdatauniversity.com/bdu-wp/bdu-course/machine-learning-cluster-analysis/\n- **PCA & CFA** http://bit.ly/1OozM3N\n\n- **R-Code** http://www.statmethods.net/advstats/factor.html\n- **Visualize** http://www.r-bloggers.com/computing-and-visualizing-pca-in-r/\n\n### Books:\n\n- **Factor Analysis** http://www.amazon.ca/Factor-Analysis-Richard-L-Gorsuch/dp/1138831999/ref=sr_1_2?ie=UTF8&qid=1444011728&sr=8-2&keywords=factor+analysis\n- **Principal Component Analysis** http://www.amazon.ca/Principal-Components-Analysis-George-Dunteman/dp/0803931042/ref=sr_1_2?ie=UTF8&qid=1444011812&sr=8-2&keywords=principal+component+analysis\n\n### Uses in Measurement:\n\n- http://scholarship.sha.cornell.edu/cgi/viewcontent.cgi?article=1618&context=articles\n- http://personal.stevens.edu/~ysakamot/719/week4/scaledevelopment.pdf\n- http://scholarship.sha.cornell.edu/cgi/viewcontent.cgi?article=1515&context=articles", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "R", "name": "ir", "language": ""}, "language_info": {"mimetype": "text/x-r-source", "pygments_lexer": "r", "file_extension": ".r", "name": "R", "codemirror_mode": "r"}}}